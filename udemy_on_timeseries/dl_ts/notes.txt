Deep Learning

# Section Overview
 - Perceptron model
 - Neural Networks
 - Keras Basics for Regression Task
 - Recurrent Neural Networks (RNN)
 - LSTM (Long short term memory units) and GRU (Gated Recurrent Unit) Neurons
 - Time Series Forecasting with RNN
 
 - Neural Networks in general tend to be  "black boxes" so it is very
   difficult to interpret them beyond their performance metrics
   (essentially, it would recieve historical data and produce a forecast
   BUT, it won't present how many lags were useful or any other relative
   info)
 - ARIMA based models are much easier to understand and work with (and
   often perform better)

# ANN (Artificial Neural Networks)
 - Actually based on biology
 - So we are going to see how to mimic biological neurons with Artificial
   Neuron -> Known as perceptron
 - Perceptron structure :
      Has multiple inputs where each input has a randomly generated
      individual weight. Input * Weight -> passed through activation function
      -> which will produce the output

      To Avoid any unforeseen errors, we introduce a "bias"
      which will evade any sort of unwanted error

      Mathematically we can represent a perceptron as following
      sigma(w_i * x_i + b) n >= i >= 0
      Multiple perceptrons will merge together into one and generate a Matrix

# Neural Networks
 - Refer to following for visual example (https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1200px-Colored_neural_network.svg.png)
 - Neural Network, literally network of neurons
 - Multiple neuron/perceptrons will form a network, working both as an
   input and output
 - Input layer: Real Values from the data
 - Hidden layer: layers between input and output layer
               : 3 or more hidden layers, it is considered as "deep network"
 - Output layer: final estimate of the output
 - As you go forward through more layers, the level of abstraction increases (NO SHIT)
 - Old activation function is a bit drastic with either 0 or 1 output, so 
   we want to build something that has a bit more spectrum of answer such as
   tanh(x), sinh(x), cosh(x), (1/ (1 + e^-(x))), etc.
   there is Rectified Linear Unit (ReLU): relatively simple with: max(0, z)
   -> best performance in many situations
 - All of these are part of a library in python, so we can just switch between
   them and don't have to build any on our own.

# Keras
 - basics of keras and build a Neural Network
 - Process of building an NN with Keras, we first define the sequential
   model object, then add layers to it.
   After, we fit the model to the training data for a chosen number of
   epochs.
   An epoch is one full pass through all the training data. Typically the
   training data is split into batches, instead of being passed all at
   once to the Network

# Deep learning with sequences (Recurrent Neural Networks)
 - Examples of Sequences:
    - Time series data
    - sentences (for NLP)
    - audio
    - car trajectories
    - music

# Normal Neuron in Feed Forward Network:
input -> aggregation of input -> activation function (i.e. ReLU : Rectified Linear Unit,
or tanh(x) function) -> Output

# Recurrent Neuron
Difference between recurrent and normal neuron is that,
unlike the normal feed which goes from input -> aggregation of input
-> activation function -> output, recurrent can recursively
pass its output from activation to the aggregation input stage. So:
input -> aggregation -> activation -> aggregation -> activation
-> aggregation -> activation ... -> activation -> output 
img: (https://www.oreilly.com/api/v2/epubs/9781492037354/files/assets/mlst_1401.png)
where it processes both activation function result and new input

- recurrent neuron can have a layer as well
- so all these "recurrence" happens within the x-axis of time (t)
- Each output at 't' is stored in memory cell.
- so memory cell would have output at t = 0, 1, 2 ...

- RNN is very flexible in terms of inputs and outputs
- basically it means, you can have a sequence of input and have singular output
  or have a singular input and have a sequence of outputs


# LSTN and GRU
# Issue with RNN is that after awhile the network will begin to "forget" the first inputs,
# as information is lost at each step going through the RNN (common on long sequence)
# so we need some "long-term memory" for the network
# Balance both short(recently trained) and long(for old trained data) term memory
# Hence, we have LSTM (Long short-term memory) cell to help address these RNN issues
# 