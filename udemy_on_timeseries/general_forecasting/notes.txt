# forecasting timeseries data

# Forecasting Procedure
 - Choose a model
 - Split data into train and test sets
 - Fit model on training set
 - Evaluate model on test set
 - Re-fit model on entire data set
 - Forecast for future data

# Section Overview
 - Introduction to Forecasting
 - ACF and PACF plots
 - AutoRegressive - AR
 - Descriptive Statistics and Tests
 - Choosing ARIMA orders
 - ARIMA based models

# Test-Train Split
 - Train Data (larger portion), Test Data (Most recent data)
 - Size of the test set is typically about 20% of the total sample
 - However, the test set's size depends on how long the sample is
   and how far ahead we want to forecast
   The Test set should "IDEALLY" be at least as large as the maximum forecast horizon required.
   Meaning, if we want to predict a month, we need as least a month's worth of data as test set 
 - Longer the forecast horizon, the lower your prediction accuracy becomes

# 3 most common evaluation metrics for regression:
 - Mean Absolute Error (MAE)
 - Mean Squared Error (MSE)
 - Root Mean Squared Error (RMSE)

y - the real value of the test data
predicted(y) - the predicted value from our forecast

MAE: 1 / n * (sum(|y_i - predicted(y_i)|))
 - It is averaging the residuals so it won't alert us if the forecast was really off for a few points
 - We want to be aware of any prediction error that are very large (even few)
 - Solution to this problem is MSE

MSE: 1 / n * sum((y_i - predicted(y_i)) ^ 2)
 - Really punishes the error of the model, so easy to notice even few errors in prediction
 - Problem : the units get squared as well (i.e. we are predicting dollars and the result is dollar squared)
 - Hence, a bit confusing
 - That is why we use RMSE (Most popular)

RMSE: root(MSE)

 - How do we evaluate a forecast for future dates?
    -> It is impossible as we don't know the actual value yet


# Stationarity and Differencing
 - Stationary, if does not exhibit trends and seasonality

# ACF and PACF
 - ACF : AutoCorrelation Function plot
 - PACF : Partial AutoCorrelation Function plot
 - So what is Correlation?
    -> Measure of the strength of the linear relationship between 2 variables
    -> ranges from -1 to +1
    -> Close it is to +1, the stronger the positive linear relationship (upward)
    -> Closer to -1, stronger the negative linear relationship (downward)
    -> Closer to 0, no Correlation (Correlation value 0 means f'(x) = 0)
 - What is AutoCorrelation?
    -> AutoCorrelation Plot shows the correlation of the eries with itself, lagged by 'x' time units
    -> So the y-axis is the correlation and the x-axis is the number of time units of lag
    -> detailed explanation : so how do we get values to plot on Correlagram (AutoCorrelation plot)
       i.e. we are plotting ACF of sales of a store.
       We plot y-axis with "Sales on day i" against x-axis with "Sales on day i - 1", and calculate
       the correlation value (i.e. 0.8). and we repeat this process by increasing alpha of 'i - alpha'
       and finding its correlation value.
       Each of the correlation value will be plotted as y-axis value with "alpha" as the x-axis value.
       This will result in a Correlagram we discussed as ACF.
 - Partial AutoCorrelation?
    -> Instead of directly using the correlation values, we calculate the residuals (the difference
       between the point and the correlation function) and plot that against the next "alpha":
       y-axis: Residuals fitted on day i - alpha, x-axis: sales on day i - alpha - 1

# In conclusion:
 - ACF describes the autocorrelation between an observation and another observation at a
   prior time step that includes direct and indirect dependence information
 - PACF only describes the direct relationship between an observation and its lag.
 - Both are used to choose ORDER PARAMETERS for ARIMA based models.


# ARIMA (AutoRegressive Integrated Moving Average)
 - Not capable of incorporating new development in data, so
   ARIMA models are not necessarily great for prediction of stock
   prices
 - Generalization of ARMA (AutoRegressive Moving Average)
 - Fitted for time series data
 - Variants: Non-Seasonal ARIMA and Seasonal ARIMA (SARIMA)
 - SARIMA can have exogenous variables and these are called SARIMAX 

 - Differencing: Used to apply ARIMA on Non-Stationary data.
   Differencing can be applied one or more times to eliminate the non-Stationarity


# Non-seasonal ARIMA:
 - Are generally denoted as ARIMA(p,d,q) where params p, d, and q are
   non-negative integers.
     -> p: AR(AutoRegression)
           A regression model that utilizes the dependent relationship between
           a current observation and observations over a previous period
     -> d: I(Integrated)
           Differencing of observations (subtracting an observation from an
           observation at the previous time step) in order to make the time
           series stationary
     -> q: MA(Moving Average)
           A model that uses the dependency between an observation and a residual
           error from a moving average model applied to lagged observations

# Stationary vs Non-stationary Data
 - Constant mean and variance
 - stationary data set will allow our model to predict that the
   mean and variance will be the same in future periods.
 - Mathematical test for Stationarity is Augumented Dickey-Fuller test
 - if NOT stationary -> transform it to stationary data (through differencing)
 - You can recursively perform differencing until you reach Stationarity
 - Each differencing step comes at the cost of losing a row of data
 
 - For seasonal data, you can also difference by a season.
 - if you had monthly data with yearly seasonality, you could difference by a
   time unit of 12, instead of just 1.
 - SARIMA, common method: combine both, taking the seasonal difference of the
   first difference.

# Assuming data is stationary:
 - How to choose p, d, q terms (2 main ways)
   1. Using ACF and PACF (by viewing the decay in these plots)
      plots are difficult to read and often even when reading them
      correctly, the best performing p, d, or q value may be different
      than what is read.
   2. Grid Search: (easy but takes time)
      RUN ARIMA based models on different combinations of p, d, and q
      and compare the models for on some evaluation metric.

      As computational power becoming cheaper and faster, its often a good IDEA
      to use the built-in automated tools that search for the correct
      p, d, and q.

# SARIMA is very similar to ARIMA, but adds another set of params (P, D, Q)
  for the seasonal component


# ARIMA Overview
 - AR - AutoRegression : indicates that the evolving variable of interest is
   regressed on its own lagged (i.e. prior) values
 - MA - Moving Average : indicates the regression error is actually a linear
   combination of error terms whose values occurred contemporaneously and
   at various times in the past
                       : uses the dependency between an observation and a
                         residual error from a moving average model applied
                         to lagged observations
 - p -> number of time lages of the AutoRegressive model.
 - d -> degree of differencing (number of times the data have had past
        values subtracted)
 - q -> order of the moving-average model